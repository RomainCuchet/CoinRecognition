{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Coin Detection\n",
        "The reason we use a jupyther notebook in gogle colab is to acces GPUs. My personal computer is an IRIS Xe wich is incompatible with torch as library only suport NVIDIAs. Go to Runtime -> Change Runtime type  "
      ],
      "metadata": {
        "id": "F5UYeNpXwQxD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set Environnement"
      ],
      "metadata": {
        "id": "c-XbaunmyZhL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "EN_PW0d6mWM7"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision.transforms as T\n",
        "from torchvision.models.detection import fasterrcnn_resnet50_fpn, FasterRCNN_ResNet50_FPN_Weights\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.optim as optim\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "from torchvision.transforms import Resize\n",
        "import os\n",
        "import time\n",
        "import xml.etree.ElementTree as ET\n",
        "from torch.utils.data import Dataset\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's import drive to have access to our dataset"
      ],
      "metadata": {
        "id": "0gm4DzHCvWdw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "KcychWuRn7kf",
        "outputId": "09fce2a3-bffa-420b-cc17-67d58b608fb3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "root_path = \"/content/drive/MyDrive/dataset\""
      ],
      "metadata": {
        "id": "NIFiXZRwoZS7"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training faster R-CNN"
      ],
      "metadata": {
        "id": "M__Qwmmoyeg-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Config:\n",
        "    def __init__(self):\n",
        "        self.LABELS = {\n",
        "            \"1_baht\": 1,\n",
        "            \"2_baht\": 2,\n",
        "            \"5_baht\": 3,\n",
        "            \"10_baht\": 4\n",
        "        }\n",
        "        self.num_epochs = 10\n",
        "        self.unfrezed_epoch = 15\n",
        "        self.img_width = 128\n",
        "        self.img_height = 128"
      ],
      "metadata": {
        "id": "c30Mfp3rCsad"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, image_dir, annot_dir,config:Config, transforms=None):\n",
        "        self.image_dir = image_dir\n",
        "        self.annot_dir = annot_dir\n",
        "        self.transforms = transforms\n",
        "        self.images = sorted(os.listdir(image_dir))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def parse_voc_xml(self, xml_file):\n",
        "        tree = ET.parse(xml_file)\n",
        "        root = tree.getroot()\n",
        "        boxes = []\n",
        "        labels = []\n",
        "\n",
        "        width = int(root.find('.//size/width').text)\n",
        "        height = int(root.find('.//size/height').text)\n",
        "\n",
        "        scale_x = config.img_width / width\n",
        "        scale_y = config.img_height / height\n",
        "        for obj in root.findall(\"object\"):\n",
        "            label = obj.find(\"name\").text\n",
        "            if label in config.LABELS:\n",
        "                labels.append(config.LABELS[label])  # Convert label name to label ID\n",
        "\n",
        "                bbox = obj.find(\"bndbox\")\n",
        "                xmin = int(int(bbox.find(\"xmin\").text)*scale_x)\n",
        "                ymin = int(int(bbox.find(\"ymin\").text)*scale_y)\n",
        "                xmax = int(int(bbox.find(\"xmax\").text)*scale_x)\n",
        "                ymax = int(int(bbox.find(\"ymax\").text)*scale_y)\n",
        "                boxes.append([xmin, ymin, xmax, ymax])\n",
        "\n",
        "        return torch.tensor(boxes, dtype=torch.float32), torch.tensor(labels, dtype=torch.int64)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = self.images[idx]\n",
        "        img_path = os.path.join(self.image_dir, img_name)\n",
        "        annot_path = os.path.join(self.annot_dir, img_name.replace(\".jpg\", \".xml\"))\n",
        "\n",
        "        # Load image\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "        # Load annotations\n",
        "        boxes, labels = self.parse_voc_xml(annot_path)\n",
        "\n",
        "        if boxes.shape[0] == 0:\n",
        "            return None\n",
        "\n",
        "        target = {\n",
        "            \"boxes\": boxes,\n",
        "            \"labels\": labels\n",
        "        }\n",
        "\n",
        "        if self.transforms:\n",
        "            img = self.transforms(img)\n",
        "        return img, target"
      ],
      "metadata": {
        "id": "D_cPrmoFqGD-"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_fn(batch):\n",
        "    batch = [b for b in batch if b is not None]  # Filter out None values\n",
        "    images, targets = zip(*batch)\n",
        "    return torch.stack(images, dim=0), {\n",
        "        \"boxes\": torch.cat([target['boxes'] for target in targets], dim=0),\n",
        "        \"labels\": torch.cat([target['labels'] for target in targets], dim=0),\n",
        "    }"
      ],
      "metadata": {
        "id": "MtwWT0MKNX39"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "x_rkkO69mWM9",
        "outputId": "189f7452-09a6-481b-d5aa-a9c287bc13a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using GPU\n",
            "Training epoch 0, time = 0.7949080467224121\n",
            "Targets in batch: [<class 'str'>, <class 'str'>]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'str' object has no attribute 'items'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-41-548f1d4ce879>\u001b[0m in \u001b[0;36m<cell line: 76>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Training epoch {epoch}, time = {time.time()-t}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_one_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Finished training epoch {epoch}/{config.num_epochs}, \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-41-548f1d4ce879>\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, optimizer, data_loader, device)\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Targets in batch: {[type(t) for t in targets]}\"\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Debugging line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m         \u001b[0;31m# Calculate loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mloss_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-41-548f1d4ce879>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Targets in batch: {[type(t) for t in targets]}\"\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Debugging line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m         \u001b[0;31m# Calculate loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mloss_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'items'"
          ]
        }
      ],
      "source": [
        "t = time.time()\n",
        "\n",
        "config = Config()\n",
        "\n",
        "# Transformations\n",
        "transform = T.Compose([\n",
        "    Resize((config.img_width, config.img_height)),  # Resize to 256x256 pixels\n",
        "    T.ToTensor(),\n",
        "])\n",
        "\n",
        "\n",
        "# Datasets and DataLoaders\n",
        "train_dataset = CustomDataset(image_dir=f\"{root_path}/train/images\", annot_dir=f\"{root_path}/train/annotations\", config=config, transforms=transform)\n",
        "valid_dataset = CustomDataset(image_dir=f\"{root_path}/valid/images\", annot_dir=f\"{root_path}/valid/annotations\",config=config, transforms=transform)\n",
        "test_dataset = CustomDataset(image_dir=f\"{root_path}/test/images\", annot_dir=f\"{root_path}/test/annotations\", config=config, transforms=transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=False,num_workers=8,collate_fn=collate_fn)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=8, shuffle=False,num_workers=8,collate_fn=collate_fn)\n",
        "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False,num_workers=8,collate_fn=collate_fn)\n",
        "\n",
        "# Load the Faster R-CNN model with ResNet-50\n",
        "model = fasterrcnn_resnet50_fpn(weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT)\n",
        "\n",
        "num_classes = len(config.LABELS) + 1  # Plus one for the background\n",
        "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
        "\n",
        "# Optimizer\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
        "\n",
        "# Move the model to GPU if available\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(\"Using GPU\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"Using CPU\")\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "# Training function\n",
        "def train_one_epoch(model, optimizer, data_loader, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for images, targets in data_loader:\n",
        "        print(f\"Targets in batch: {[type(t) for t in targets]}\")  # Debugging line\n",
        "        print(f\"Targets in batch: {[t for t in targets]}\")  # Debugging line\n",
        "        images = [img.to(device) for img in images]\n",
        "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "        # Calculate loss\n",
        "        loss_dict = model(images, targets)\n",
        "        losses = sum(loss for loss in loss_dict.values())\n",
        "        total_loss += losses.item()\n",
        "\n",
        "        # Backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        losses.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    return total_loss / len(data_loader)\n",
        "\n",
        "# Evaluation function\n",
        "@torch.no_grad()\n",
        "def evaluate(model, data_loader, device):\n",
        "    model.eval()\n",
        "    for images, targets in data_loader:\n",
        "        images = [img.to(device) for img in images]\n",
        "        outputs = model(images)\n",
        "        # Perform evaluation metrics if necessary\n",
        "\n",
        "for param in model.backbone.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Main training loop\n",
        "config.num_epochs = 10\n",
        "config.unfrezed_epoch = 15\n",
        "for epoch in range(config.num_epochs):\n",
        "    if epoch==config.unfrezed_epoch:\n",
        "      for param in model.backbone.body.layer4.parameters():\n",
        "        param.requires_grad = True\n",
        "    print(f\"Training epoch {epoch}, time = {time.time()-t}\")\n",
        "\n",
        "    train_loss = train_one_epoch(model, optimizer, train_loader, device)\n",
        "\n",
        "    print(f\"Finished training epoch {epoch}/{config.num_epochs}, \")\n",
        "    # Validate on validation set\n",
        "    evaluate(model, valid_loader, device)\n",
        "    print(f\"epoch {epoch}/{config.num_epochs}, {time.time()-t}s\")\n",
        "model._save_to_state_dict(\"model.pth\")\n",
        "print(\"Training completed in \", time.time()-t)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.3"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}